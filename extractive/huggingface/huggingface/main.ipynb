{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc2a765",
   "metadata": {},
   "source": [
    "# Question answering (extractive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c976d9d38952e",
   "metadata": {},
   "source": [
    "**Reference**: https://huggingface.co/learn/nlp-course/chapter7/7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe103df",
   "metadata": {},
   "source": [
    "> üí° Encoder-only models like BERT tend to be great at extracting answers to factoid questions like ‚ÄúWho invented the Transformer architecture?‚Äù but fare poorly when given open-ended questions like ‚ÄúWhy is the sky blue?‚Äù \\\n",
    "In these more challenging cases, encoder-decoder models like T5 and BART are typically used to synthesize the information in a way that‚Äôs quite similar to text summarization. \\\n",
    "If you‚Äôre interested in this type of generative question answering, we recommend checking out our demo based on the ELI5 dataset.\n",
    "\n",
    "- Generative question answering demo: [https://yjernite.github.io/lfqa.html](https://yjernite.github.io/lfqa.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255a3eb",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "1. Most academic benchmark for extractive question answering: [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "    - [SQuAD v2](https://huggingface.co/datasets/squad_v2): includes questions that don't hanve an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55656f67",
   "metadata": {},
   "source": [
    "## The SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e2d119a4f4142d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:08.609046300Z",
     "start_time": "2023-12-20T06:02:08.205547500Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff0de0df3d26b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:10.743399800Z",
     "start_time": "2023-12-20T06:02:10.738033600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "- Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "- Answers:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "print(\"- Context: \",  raw_datasets[\"train\"][0][\"context\"])\n",
    "print(\"- Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
    "print(\"- Answers: \",  raw_datasets[\"train\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fd65d",
   "metadata": {},
   "source": [
    "- `answers`: This is the format that will be expected by the squad metric during evaluation.\n",
    "    ```\n",
    "    {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
    "    ```\n",
    "    - `answer_start`: character index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6869569c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "context, answers = raw_datasets[\"train\"][0][\"context\"], raw_datasets[\"train\"][0][\"answers\"]\n",
    "\n",
    "print(answers)\n",
    "print(context[answers['answer_start'][0] : answers['answer_start'][0] + len(answers['text'][0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90240c4b",
   "metadata": {},
   "source": [
    "During training, there is only **one** possible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f15248b22f551c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:10.750291600Z",
     "start_time": "2023-12-20T06:02:10.742316200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].filter(lambda x: len(x['answers']['text']) != 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd07372",
   "metadata": {},
   "source": [
    "For evaluation, however, there are several possible answers for each sample, which may be the same or different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c60ece4ba0c5fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:10.805864800Z",
     "start_time": "2023-12-20T06:02:10.751866100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}\n",
      "{'text': ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], 'answer_start': [403, 355, 355]}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['validation'][0]['answers'])\n",
    "print(raw_datasets['validation'][2]['answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff34368",
   "metadata": {},
   "source": [
    "some of the questions have several possible answers, and this script(ü§ó Datasets metric) will **compare a predicted answer to all the acceptable answers and take the best score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c72517fdfcc1271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:10.806381700Z",
     "start_time": "2023-12-20T06:02:10.802230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Where did Super Bowl 50 take place?\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['validation'][2]['context'])\n",
    "print(raw_datasets['validation'][2]['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995692e",
   "metadata": {},
   "source": [
    "## Procesing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1097db7",
   "metadata": {},
   "source": [
    "- Supported models: [https://huggingface.co/docs/transformers/index#supported-models-and-frameworks](https://huggingface.co/docs/transformers/index#supported-models-and-frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb6a3397212be1ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:12.971218500Z",
     "start_time": "2023-12-20T06:02:10.802230Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8adfb7f",
   "metadata": {},
   "source": [
    "they have a Python tokenizer (called ‚Äú**slow**‚Äù). A ‚Äú**fast**‚Äù tokenizer backed by the ü§ó Tokenizers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c0a0c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651d8b5",
   "metadata": {},
   "source": [
    "- `BERT` input sentence format\n",
    "    ```\n",
    "    [CLS] question [SEP] context [SEP]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20f7b7a333324e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:13.021601300Z",
     "start_time": "2023-12-20T06:02:12.973298200Z"
    }
   },
   "outputs": [],
   "source": [
    "context  = raw_datasets['train'][0]['context']\n",
    "question = raw_datasets['train'][0]['question']\n",
    "answer   = raw_datasets['train'][0]['answers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d65e8",
   "metadata": {},
   "source": [
    "Task image\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5defd13c8aa37a3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:13.022121800Z",
     "start_time": "2023-12-20T06:02:13.021601300Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question, context,\n",
    "    max_length=100,  # the number of tokens in a sentence\n",
    "    truncation='only_second',\n",
    "    stride=50,  # the number of overlapping tokens\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "108ae749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86a4a875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f12e9977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c3e2fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:13.022647300Z",
     "start_time": "2023-12-20T06:02:13.021601300Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fa940b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sents = len(inputs['input_ids'])\n",
    "assert n_sents == len(inputs['overflow_to_sample_mapping'])\n",
    "n_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc25d8ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- sentence 0: [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
      "- length: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>To</td>\n",
       "      <td>whom</td>\n",
       "      <td>did</td>\n",
       "      <td>the</td>\n",
       "      <td>Virgin</td>\n",
       "      <td>Mary</td>\n",
       "      <td>allegedly</td>\n",
       "      <td>appear</td>\n",
       "      <td>in</td>\n",
       "      <td>1858</td>\n",
       "      <td>in</td>\n",
       "      <td>Lou</td>\n",
       "      <td>##rdes</td>\n",
       "      <td>France</td>\n",
       "      <td>?</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>Architectural</td>\n",
       "      <td>##ly</td>\n",
       "      <td>,</td>\n",
       "      <td>the</td>\n",
       "      <td>school</td>\n",
       "      <td>has</td>\n",
       "      <td>a</td>\n",
       "      <td>Catholic</td>\n",
       "      <td>character</td>\n",
       "      <td>.</td>\n",
       "      <td>At</td>\n",
       "      <td>##op</td>\n",
       "      <td>the</td>\n",
       "      <td>Main</td>\n",
       "      <td>Building</td>\n",
       "      <td>'</td>\n",
       "      <td>s</td>\n",
       "      <td>gold</td>\n",
       "      <td>dome</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>golden</td>\n",
       "      <td>statue</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>Virgin</td>\n",
       "      <td>Mary</td>\n",
       "      <td>.</td>\n",
       "      <td>Immediately</td>\n",
       "      <td>in</td>\n",
       "      <td>front</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>Main</td>\n",
       "      <td>Building</td>\n",
       "      <td>and</td>\n",
       "      <td>facing</td>\n",
       "      <td>it</td>\n",
       "      <td>,</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>copper</td>\n",
       "      <td>statue</td>\n",
       "      <td>of</td>\n",
       "      <td>Christ</td>\n",
       "      <td>with</td>\n",
       "      <td>arms</td>\n",
       "      <td>up</td>\n",
       "      <td>##rai</td>\n",
       "      <td>##sed</td>\n",
       "      <td>with</td>\n",
       "      <td>the</td>\n",
       "      <td>legend</td>\n",
       "      <td>\"</td>\n",
       "      <td>V</td>\n",
       "      <td>##eni</td>\n",
       "      <td>##te</td>\n",
       "      <td>Ad</td>\n",
       "      <td>Me</td>\n",
       "      <td>O</td>\n",
       "      <td>##m</td>\n",
       "      <td>##nes</td>\n",
       "      <td>\"</td>\n",
       "      <td>.</td>\n",
       "      <td>Next</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>Main</td>\n",
       "      <td>Building</td>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "      <td>Basilica</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>Sacred</td>\n",
       "      <td>Heart</td>\n",
       "      <td>.</td>\n",
       "      <td>Immediately</td>\n",
       "      <td>behind</td>\n",
       "      <td>the</td>\n",
       "      <td>b</td>\n",
       "      <td>##asi</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <td>101</td>\n",
       "      <td>1706</td>\n",
       "      <td>2292</td>\n",
       "      <td>1225</td>\n",
       "      <td>1103</td>\n",
       "      <td>6567</td>\n",
       "      <td>2090</td>\n",
       "      <td>9273</td>\n",
       "      <td>2845</td>\n",
       "      <td>1107</td>\n",
       "      <td>8109</td>\n",
       "      <td>1107</td>\n",
       "      <td>10111</td>\n",
       "      <td>20500</td>\n",
       "      <td>1699</td>\n",
       "      <td>136</td>\n",
       "      <td>102</td>\n",
       "      <td>22182</td>\n",
       "      <td>1193</td>\n",
       "      <td>117</td>\n",
       "      <td>1103</td>\n",
       "      <td>1278</td>\n",
       "      <td>1144</td>\n",
       "      <td>170</td>\n",
       "      <td>2336</td>\n",
       "      <td>1959</td>\n",
       "      <td>119</td>\n",
       "      <td>1335</td>\n",
       "      <td>4184</td>\n",
       "      <td>1103</td>\n",
       "      <td>4304</td>\n",
       "      <td>4334</td>\n",
       "      <td>112</td>\n",
       "      <td>188</td>\n",
       "      <td>2284</td>\n",
       "      <td>10945</td>\n",
       "      <td>1110</td>\n",
       "      <td>170</td>\n",
       "      <td>5404</td>\n",
       "      <td>5921</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>6567</td>\n",
       "      <td>2090</td>\n",
       "      <td>119</td>\n",
       "      <td>13301</td>\n",
       "      <td>1107</td>\n",
       "      <td>1524</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>4304</td>\n",
       "      <td>4334</td>\n",
       "      <td>1105</td>\n",
       "      <td>4749</td>\n",
       "      <td>1122</td>\n",
       "      <td>117</td>\n",
       "      <td>1110</td>\n",
       "      <td>170</td>\n",
       "      <td>7335</td>\n",
       "      <td>5921</td>\n",
       "      <td>1104</td>\n",
       "      <td>4028</td>\n",
       "      <td>1114</td>\n",
       "      <td>1739</td>\n",
       "      <td>1146</td>\n",
       "      <td>14089</td>\n",
       "      <td>5591</td>\n",
       "      <td>1114</td>\n",
       "      <td>1103</td>\n",
       "      <td>7051</td>\n",
       "      <td>107</td>\n",
       "      <td>159</td>\n",
       "      <td>21462</td>\n",
       "      <td>1566</td>\n",
       "      <td>24930</td>\n",
       "      <td>2508</td>\n",
       "      <td>152</td>\n",
       "      <td>1306</td>\n",
       "      <td>3965</td>\n",
       "      <td>107</td>\n",
       "      <td>119</td>\n",
       "      <td>5893</td>\n",
       "      <td>1106</td>\n",
       "      <td>1103</td>\n",
       "      <td>4304</td>\n",
       "      <td>4334</td>\n",
       "      <td>1110</td>\n",
       "      <td>1103</td>\n",
       "      <td>19349</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>11373</td>\n",
       "      <td>4641</td>\n",
       "      <td>119</td>\n",
       "      <td>13301</td>\n",
       "      <td>1481</td>\n",
       "      <td>1103</td>\n",
       "      <td>171</td>\n",
       "      <td>17506</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_type_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offset_mapping</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(3, 7)</td>\n",
       "      <td>(8, 11)</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>(16, 22)</td>\n",
       "      <td>(23, 27)</td>\n",
       "      <td>(28, 37)</td>\n",
       "      <td>(38, 44)</td>\n",
       "      <td>(45, 47)</td>\n",
       "      <td>(48, 52)</td>\n",
       "      <td>(53, 55)</td>\n",
       "      <td>(56, 59)</td>\n",
       "      <td>(59, 63)</td>\n",
       "      <td>(64, 70)</td>\n",
       "      <td>(70, 71)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(0, 13)</td>\n",
       "      <td>(13, 15)</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>(17, 20)</td>\n",
       "      <td>(21, 27)</td>\n",
       "      <td>(28, 31)</td>\n",
       "      <td>(32, 33)</td>\n",
       "      <td>(34, 42)</td>\n",
       "      <td>(43, 52)</td>\n",
       "      <td>(52, 53)</td>\n",
       "      <td>(54, 56)</td>\n",
       "      <td>(56, 58)</td>\n",
       "      <td>(59, 62)</td>\n",
       "      <td>(63, 67)</td>\n",
       "      <td>(68, 76)</td>\n",
       "      <td>(76, 77)</td>\n",
       "      <td>(77, 78)</td>\n",
       "      <td>(79, 83)</td>\n",
       "      <td>(84, 88)</td>\n",
       "      <td>(89, 91)</td>\n",
       "      <td>(92, 93)</td>\n",
       "      <td>(94, 100)</td>\n",
       "      <td>(101, 107)</td>\n",
       "      <td>(108, 110)</td>\n",
       "      <td>(111, 114)</td>\n",
       "      <td>(115, 121)</td>\n",
       "      <td>(122, 126)</td>\n",
       "      <td>(126, 127)</td>\n",
       "      <td>(128, 139)</td>\n",
       "      <td>(140, 142)</td>\n",
       "      <td>(143, 148)</td>\n",
       "      <td>(149, 151)</td>\n",
       "      <td>(152, 155)</td>\n",
       "      <td>(156, 160)</td>\n",
       "      <td>(161, 169)</td>\n",
       "      <td>(170, 173)</td>\n",
       "      <td>(174, 180)</td>\n",
       "      <td>(181, 183)</td>\n",
       "      <td>(183, 184)</td>\n",
       "      <td>(185, 187)</td>\n",
       "      <td>(188, 189)</td>\n",
       "      <td>(190, 196)</td>\n",
       "      <td>(197, 203)</td>\n",
       "      <td>(204, 206)</td>\n",
       "      <td>(207, 213)</td>\n",
       "      <td>(214, 218)</td>\n",
       "      <td>(219, 223)</td>\n",
       "      <td>(224, 226)</td>\n",
       "      <td>(226, 229)</td>\n",
       "      <td>(229, 232)</td>\n",
       "      <td>(233, 237)</td>\n",
       "      <td>(238, 241)</td>\n",
       "      <td>(242, 248)</td>\n",
       "      <td>(249, 250)</td>\n",
       "      <td>(250, 251)</td>\n",
       "      <td>(251, 254)</td>\n",
       "      <td>(254, 256)</td>\n",
       "      <td>(257, 259)</td>\n",
       "      <td>(260, 262)</td>\n",
       "      <td>(263, 264)</td>\n",
       "      <td>(264, 265)</td>\n",
       "      <td>(265, 268)</td>\n",
       "      <td>(268, 269)</td>\n",
       "      <td>(269, 270)</td>\n",
       "      <td>(271, 275)</td>\n",
       "      <td>(276, 278)</td>\n",
       "      <td>(279, 282)</td>\n",
       "      <td>(283, 287)</td>\n",
       "      <td>(288, 296)</td>\n",
       "      <td>(297, 299)</td>\n",
       "      <td>(300, 303)</td>\n",
       "      <td>(304, 312)</td>\n",
       "      <td>(313, 315)</td>\n",
       "      <td>(316, 319)</td>\n",
       "      <td>(320, 326)</td>\n",
       "      <td>(327, 332)</td>\n",
       "      <td>(332, 333)</td>\n",
       "      <td>(334, 345)</td>\n",
       "      <td>(346, 352)</td>\n",
       "      <td>(353, 356)</td>\n",
       "      <td>(357, 358)</td>\n",
       "      <td>(358, 361)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1       2        3         4         5         6   \\\n",
       "token            [CLS]      To    whom      did       the    Virgin      Mary   \n",
       "token_id           101    1706    2292     1225      1103      6567      2090   \n",
       "token_type_id        0       0       0        0         0         0         0   \n",
       "attention_mask       1       1       1        1         1         1         1   \n",
       "offset_mapping  (0, 0)  (0, 2)  (3, 7)  (8, 11)  (12, 15)  (16, 22)  (23, 27)   \n",
       "\n",
       "                       7         8         9         10        11        12  \\\n",
       "token           allegedly    appear        in      1858        in       Lou   \n",
       "token_id             9273      2845      1107      8109      1107     10111   \n",
       "token_type_id           0         0         0         0         0         0   \n",
       "attention_mask          1         1         1         1         1         1   \n",
       "offset_mapping   (28, 37)  (38, 44)  (45, 47)  (48, 52)  (53, 55)  (56, 59)   \n",
       "\n",
       "                      13        14        15      16             17        18  \\\n",
       "token             ##rdes    France         ?   [SEP]  Architectural      ##ly   \n",
       "token_id           20500      1699       136     102          22182      1193   \n",
       "token_type_id          0         0         0       0              1         1   \n",
       "attention_mask         1         1         1       1              1         1   \n",
       "offset_mapping  (59, 63)  (64, 70)  (70, 71)  (0, 0)        (0, 13)  (13, 15)   \n",
       "\n",
       "                      19        20        21        22        23        24  \\\n",
       "token                  ,       the    school       has         a  Catholic   \n",
       "token_id             117      1103      1278      1144       170      2336   \n",
       "token_type_id          1         1         1         1         1         1   \n",
       "attention_mask         1         1         1         1         1         1   \n",
       "offset_mapping  (15, 16)  (17, 20)  (21, 27)  (28, 31)  (32, 33)  (34, 42)   \n",
       "\n",
       "                       25        26        27        28        29        30  \\\n",
       "token           character         .        At      ##op       the      Main   \n",
       "token_id             1959       119      1335      4184      1103      4304   \n",
       "token_type_id           1         1         1         1         1         1   \n",
       "attention_mask          1         1         1         1         1         1   \n",
       "offset_mapping   (43, 52)  (52, 53)  (54, 56)  (56, 58)  (59, 62)  (63, 67)   \n",
       "\n",
       "                      31        32        33        34        35        36  \\\n",
       "token           Building         '         s      gold      dome        is   \n",
       "token_id            4334       112       188      2284     10945      1110   \n",
       "token_type_id          1         1         1         1         1         1   \n",
       "attention_mask         1         1         1         1         1         1   \n",
       "offset_mapping  (68, 76)  (76, 77)  (77, 78)  (79, 83)  (84, 88)  (89, 91)   \n",
       "\n",
       "                      37         38          39          40          41  \\\n",
       "token                  a     golden      statue          of         the   \n",
       "token_id             170       5404        5921        1104        1103   \n",
       "token_type_id          1          1           1           1           1   \n",
       "attention_mask         1          1           1           1           1   \n",
       "offset_mapping  (92, 93)  (94, 100)  (101, 107)  (108, 110)  (111, 114)   \n",
       "\n",
       "                        42          43          44           45          46  \\\n",
       "token               Virgin        Mary           .  Immediately          in   \n",
       "token_id              6567        2090         119        13301        1107   \n",
       "token_type_id            1           1           1            1           1   \n",
       "attention_mask           1           1           1            1           1   \n",
       "offset_mapping  (115, 121)  (122, 126)  (126, 127)   (128, 139)  (140, 142)   \n",
       "\n",
       "                        47          48          49          50          51  \\\n",
       "token                front          of         the        Main    Building   \n",
       "token_id              1524        1104        1103        4304        4334   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (143, 148)  (149, 151)  (152, 155)  (156, 160)  (161, 169)   \n",
       "\n",
       "                        52          53          54          55          56  \\\n",
       "token                  and      facing          it           ,          is   \n",
       "token_id              1105        4749        1122         117        1110   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (170, 173)  (174, 180)  (181, 183)  (183, 184)  (185, 187)   \n",
       "\n",
       "                        57          58          59          60          61  \\\n",
       "token                    a      copper      statue          of      Christ   \n",
       "token_id               170        7335        5921        1104        4028   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (188, 189)  (190, 196)  (197, 203)  (204, 206)  (207, 213)   \n",
       "\n",
       "                        62          63          64          65          66  \\\n",
       "token                 with        arms          up       ##rai       ##sed   \n",
       "token_id              1114        1739        1146       14089        5591   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (214, 218)  (219, 223)  (224, 226)  (226, 229)  (229, 232)   \n",
       "\n",
       "                        67          68          69          70          71  \\\n",
       "token                 with         the      legend           \"           V   \n",
       "token_id              1114        1103        7051         107         159   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (233, 237)  (238, 241)  (242, 248)  (249, 250)  (250, 251)   \n",
       "\n",
       "                        72          73          74          75          76  \\\n",
       "token                ##eni        ##te          Ad          Me           O   \n",
       "token_id             21462        1566       24930        2508         152   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (251, 254)  (254, 256)  (257, 259)  (260, 262)  (263, 264)   \n",
       "\n",
       "                        77          78          79          80          81  \\\n",
       "token                  ##m       ##nes           \"           .        Next   \n",
       "token_id              1306        3965         107         119        5893   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (264, 265)  (265, 268)  (268, 269)  (269, 270)  (271, 275)   \n",
       "\n",
       "                        82          83          84          85          86  \\\n",
       "token                   to         the        Main    Building          is   \n",
       "token_id              1106        1103        4304        4334        1110   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (276, 278)  (279, 282)  (283, 287)  (288, 296)  (297, 299)   \n",
       "\n",
       "                        87          88          89          90          91  \\\n",
       "token                  the    Basilica          of         the      Sacred   \n",
       "token_id              1103       19349        1104        1103       11373   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (300, 303)  (304, 312)  (313, 315)  (316, 319)  (320, 326)   \n",
       "\n",
       "                        92          93           94          95          96  \\\n",
       "token                Heart           .  Immediately      behind         the   \n",
       "token_id              4641         119        13301        1481        1103   \n",
       "token_type_id            1           1            1           1           1   \n",
       "attention_mask           1           1            1           1           1   \n",
       "offset_mapping  (327, 332)  (332, 333)   (334, 345)  (346, 352)  (353, 356)   \n",
       "\n",
       "                        97          98      99  \n",
       "token                    b       ##asi   [SEP]  \n",
       "token_id               171       17506     102  \n",
       "token_type_id            1           1       1  \n",
       "attention_mask           1           1       1  \n",
       "offset_mapping  (357, 358)  (358, 361)  (0, 0)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- sentence 1: [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
      "- length: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>To</td>\n",
       "      <td>whom</td>\n",
       "      <td>did</td>\n",
       "      <td>the</td>\n",
       "      <td>Virgin</td>\n",
       "      <td>Mary</td>\n",
       "      <td>allegedly</td>\n",
       "      <td>appear</td>\n",
       "      <td>in</td>\n",
       "      <td>1858</td>\n",
       "      <td>in</td>\n",
       "      <td>Lou</td>\n",
       "      <td>##rdes</td>\n",
       "      <td>France</td>\n",
       "      <td>?</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>the</td>\n",
       "      <td>Main</td>\n",
       "      <td>Building</td>\n",
       "      <td>and</td>\n",
       "      <td>facing</td>\n",
       "      <td>it</td>\n",
       "      <td>,</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>copper</td>\n",
       "      <td>statue</td>\n",
       "      <td>of</td>\n",
       "      <td>Christ</td>\n",
       "      <td>with</td>\n",
       "      <td>arms</td>\n",
       "      <td>up</td>\n",
       "      <td>##rai</td>\n",
       "      <td>##sed</td>\n",
       "      <td>with</td>\n",
       "      <td>the</td>\n",
       "      <td>legend</td>\n",
       "      <td>\"</td>\n",
       "      <td>V</td>\n",
       "      <td>##eni</td>\n",
       "      <td>##te</td>\n",
       "      <td>Ad</td>\n",
       "      <td>Me</td>\n",
       "      <td>O</td>\n",
       "      <td>##m</td>\n",
       "      <td>##nes</td>\n",
       "      <td>\"</td>\n",
       "      <td>.</td>\n",
       "      <td>Next</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>Main</td>\n",
       "      <td>Building</td>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "      <td>Basilica</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>Sacred</td>\n",
       "      <td>Heart</td>\n",
       "      <td>.</td>\n",
       "      <td>Immediately</td>\n",
       "      <td>behind</td>\n",
       "      <td>the</td>\n",
       "      <td>b</td>\n",
       "      <td>##asi</td>\n",
       "      <td>##lica</td>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "      <td>G</td>\n",
       "      <td>##rot</td>\n",
       "      <td>##to</td>\n",
       "      <td>,</td>\n",
       "      <td>a</td>\n",
       "      <td>Marian</td>\n",
       "      <td>place</td>\n",
       "      <td>of</td>\n",
       "      <td>prayer</td>\n",
       "      <td>and</td>\n",
       "      <td>reflection</td>\n",
       "      <td>.</td>\n",
       "      <td>It</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>replica</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>g</td>\n",
       "      <td>##rot</td>\n",
       "      <td>##to</td>\n",
       "      <td>at</td>\n",
       "      <td>Lou</td>\n",
       "      <td>##rdes</td>\n",
       "      <td>,</td>\n",
       "      <td>France</td>\n",
       "      <td>where</td>\n",
       "      <td>the</td>\n",
       "      <td>Virgin</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <td>101</td>\n",
       "      <td>1706</td>\n",
       "      <td>2292</td>\n",
       "      <td>1225</td>\n",
       "      <td>1103</td>\n",
       "      <td>6567</td>\n",
       "      <td>2090</td>\n",
       "      <td>9273</td>\n",
       "      <td>2845</td>\n",
       "      <td>1107</td>\n",
       "      <td>8109</td>\n",
       "      <td>1107</td>\n",
       "      <td>10111</td>\n",
       "      <td>20500</td>\n",
       "      <td>1699</td>\n",
       "      <td>136</td>\n",
       "      <td>102</td>\n",
       "      <td>1103</td>\n",
       "      <td>4304</td>\n",
       "      <td>4334</td>\n",
       "      <td>1105</td>\n",
       "      <td>4749</td>\n",
       "      <td>1122</td>\n",
       "      <td>117</td>\n",
       "      <td>1110</td>\n",
       "      <td>170</td>\n",
       "      <td>7335</td>\n",
       "      <td>5921</td>\n",
       "      <td>1104</td>\n",
       "      <td>4028</td>\n",
       "      <td>1114</td>\n",
       "      <td>1739</td>\n",
       "      <td>1146</td>\n",
       "      <td>14089</td>\n",
       "      <td>5591</td>\n",
       "      <td>1114</td>\n",
       "      <td>1103</td>\n",
       "      <td>7051</td>\n",
       "      <td>107</td>\n",
       "      <td>159</td>\n",
       "      <td>21462</td>\n",
       "      <td>1566</td>\n",
       "      <td>24930</td>\n",
       "      <td>2508</td>\n",
       "      <td>152</td>\n",
       "      <td>1306</td>\n",
       "      <td>3965</td>\n",
       "      <td>107</td>\n",
       "      <td>119</td>\n",
       "      <td>5893</td>\n",
       "      <td>1106</td>\n",
       "      <td>1103</td>\n",
       "      <td>4304</td>\n",
       "      <td>4334</td>\n",
       "      <td>1110</td>\n",
       "      <td>1103</td>\n",
       "      <td>19349</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>11373</td>\n",
       "      <td>4641</td>\n",
       "      <td>119</td>\n",
       "      <td>13301</td>\n",
       "      <td>1481</td>\n",
       "      <td>1103</td>\n",
       "      <td>171</td>\n",
       "      <td>17506</td>\n",
       "      <td>9538</td>\n",
       "      <td>1110</td>\n",
       "      <td>1103</td>\n",
       "      <td>144</td>\n",
       "      <td>10595</td>\n",
       "      <td>2430</td>\n",
       "      <td>117</td>\n",
       "      <td>170</td>\n",
       "      <td>14789</td>\n",
       "      <td>1282</td>\n",
       "      <td>1104</td>\n",
       "      <td>8070</td>\n",
       "      <td>1105</td>\n",
       "      <td>9284</td>\n",
       "      <td>119</td>\n",
       "      <td>1135</td>\n",
       "      <td>1110</td>\n",
       "      <td>170</td>\n",
       "      <td>16498</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>176</td>\n",
       "      <td>10595</td>\n",
       "      <td>2430</td>\n",
       "      <td>1120</td>\n",
       "      <td>10111</td>\n",
       "      <td>20500</td>\n",
       "      <td>117</td>\n",
       "      <td>1699</td>\n",
       "      <td>1187</td>\n",
       "      <td>1103</td>\n",
       "      <td>6567</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_type_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offset_mapping</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(3, 7)</td>\n",
       "      <td>(8, 11)</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>(16, 22)</td>\n",
       "      <td>(23, 27)</td>\n",
       "      <td>(28, 37)</td>\n",
       "      <td>(38, 44)</td>\n",
       "      <td>(45, 47)</td>\n",
       "      <td>(48, 52)</td>\n",
       "      <td>(53, 55)</td>\n",
       "      <td>(56, 59)</td>\n",
       "      <td>(59, 63)</td>\n",
       "      <td>(64, 70)</td>\n",
       "      <td>(70, 71)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(152, 155)</td>\n",
       "      <td>(156, 160)</td>\n",
       "      <td>(161, 169)</td>\n",
       "      <td>(170, 173)</td>\n",
       "      <td>(174, 180)</td>\n",
       "      <td>(181, 183)</td>\n",
       "      <td>(183, 184)</td>\n",
       "      <td>(185, 187)</td>\n",
       "      <td>(188, 189)</td>\n",
       "      <td>(190, 196)</td>\n",
       "      <td>(197, 203)</td>\n",
       "      <td>(204, 206)</td>\n",
       "      <td>(207, 213)</td>\n",
       "      <td>(214, 218)</td>\n",
       "      <td>(219, 223)</td>\n",
       "      <td>(224, 226)</td>\n",
       "      <td>(226, 229)</td>\n",
       "      <td>(229, 232)</td>\n",
       "      <td>(233, 237)</td>\n",
       "      <td>(238, 241)</td>\n",
       "      <td>(242, 248)</td>\n",
       "      <td>(249, 250)</td>\n",
       "      <td>(250, 251)</td>\n",
       "      <td>(251, 254)</td>\n",
       "      <td>(254, 256)</td>\n",
       "      <td>(257, 259)</td>\n",
       "      <td>(260, 262)</td>\n",
       "      <td>(263, 264)</td>\n",
       "      <td>(264, 265)</td>\n",
       "      <td>(265, 268)</td>\n",
       "      <td>(268, 269)</td>\n",
       "      <td>(269, 270)</td>\n",
       "      <td>(271, 275)</td>\n",
       "      <td>(276, 278)</td>\n",
       "      <td>(279, 282)</td>\n",
       "      <td>(283, 287)</td>\n",
       "      <td>(288, 296)</td>\n",
       "      <td>(297, 299)</td>\n",
       "      <td>(300, 303)</td>\n",
       "      <td>(304, 312)</td>\n",
       "      <td>(313, 315)</td>\n",
       "      <td>(316, 319)</td>\n",
       "      <td>(320, 326)</td>\n",
       "      <td>(327, 332)</td>\n",
       "      <td>(332, 333)</td>\n",
       "      <td>(334, 345)</td>\n",
       "      <td>(346, 352)</td>\n",
       "      <td>(353, 356)</td>\n",
       "      <td>(357, 358)</td>\n",
       "      <td>(358, 361)</td>\n",
       "      <td>(361, 365)</td>\n",
       "      <td>(366, 368)</td>\n",
       "      <td>(369, 372)</td>\n",
       "      <td>(373, 374)</td>\n",
       "      <td>(374, 377)</td>\n",
       "      <td>(377, 379)</td>\n",
       "      <td>(379, 380)</td>\n",
       "      <td>(381, 382)</td>\n",
       "      <td>(383, 389)</td>\n",
       "      <td>(390, 395)</td>\n",
       "      <td>(396, 398)</td>\n",
       "      <td>(399, 405)</td>\n",
       "      <td>(406, 409)</td>\n",
       "      <td>(410, 420)</td>\n",
       "      <td>(420, 421)</td>\n",
       "      <td>(422, 424)</td>\n",
       "      <td>(425, 427)</td>\n",
       "      <td>(428, 429)</td>\n",
       "      <td>(430, 437)</td>\n",
       "      <td>(438, 440)</td>\n",
       "      <td>(441, 444)</td>\n",
       "      <td>(445, 446)</td>\n",
       "      <td>(446, 449)</td>\n",
       "      <td>(449, 451)</td>\n",
       "      <td>(452, 454)</td>\n",
       "      <td>(455, 458)</td>\n",
       "      <td>(458, 462)</td>\n",
       "      <td>(462, 463)</td>\n",
       "      <td>(464, 470)</td>\n",
       "      <td>(471, 476)</td>\n",
       "      <td>(477, 480)</td>\n",
       "      <td>(481, 487)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1       2        3         4         5         6   \\\n",
       "token            [CLS]      To    whom      did       the    Virgin      Mary   \n",
       "token_id           101    1706    2292     1225      1103      6567      2090   \n",
       "token_type_id        0       0       0        0         0         0         0   \n",
       "attention_mask       1       1       1        1         1         1         1   \n",
       "offset_mapping  (0, 0)  (0, 2)  (3, 7)  (8, 11)  (12, 15)  (16, 22)  (23, 27)   \n",
       "\n",
       "                       7         8         9         10        11        12  \\\n",
       "token           allegedly    appear        in      1858        in       Lou   \n",
       "token_id             9273      2845      1107      8109      1107     10111   \n",
       "token_type_id           0         0         0         0         0         0   \n",
       "attention_mask          1         1         1         1         1         1   \n",
       "offset_mapping   (28, 37)  (38, 44)  (45, 47)  (48, 52)  (53, 55)  (56, 59)   \n",
       "\n",
       "                      13        14        15      16          17          18  \\\n",
       "token             ##rdes    France         ?   [SEP]         the        Main   \n",
       "token_id           20500      1699       136     102        1103        4304   \n",
       "token_type_id          0         0         0       0           1           1   \n",
       "attention_mask         1         1         1       1           1           1   \n",
       "offset_mapping  (59, 63)  (64, 70)  (70, 71)  (0, 0)  (152, 155)  (156, 160)   \n",
       "\n",
       "                        19          20          21          22          23  \\\n",
       "token             Building         and      facing          it           ,   \n",
       "token_id              4334        1105        4749        1122         117   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (161, 169)  (170, 173)  (174, 180)  (181, 183)  (183, 184)   \n",
       "\n",
       "                        24          25          26          27          28  \\\n",
       "token                   is           a      copper      statue          of   \n",
       "token_id              1110         170        7335        5921        1104   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (185, 187)  (188, 189)  (190, 196)  (197, 203)  (204, 206)   \n",
       "\n",
       "                        29          30          31          32          33  \\\n",
       "token               Christ        with        arms          up       ##rai   \n",
       "token_id              4028        1114        1739        1146       14089   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (207, 213)  (214, 218)  (219, 223)  (224, 226)  (226, 229)   \n",
       "\n",
       "                        34          35          36          37          38  \\\n",
       "token                ##sed        with         the      legend           \"   \n",
       "token_id              5591        1114        1103        7051         107   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (229, 232)  (233, 237)  (238, 241)  (242, 248)  (249, 250)   \n",
       "\n",
       "                        39          40          41          42          43  \\\n",
       "token                    V       ##eni        ##te          Ad          Me   \n",
       "token_id               159       21462        1566       24930        2508   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (250, 251)  (251, 254)  (254, 256)  (257, 259)  (260, 262)   \n",
       "\n",
       "                        44          45          46          47          48  \\\n",
       "token                    O         ##m       ##nes           \"           .   \n",
       "token_id               152        1306        3965         107         119   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (263, 264)  (264, 265)  (265, 268)  (268, 269)  (269, 270)   \n",
       "\n",
       "                        49          50          51          52          53  \\\n",
       "token                 Next          to         the        Main    Building   \n",
       "token_id              5893        1106        1103        4304        4334   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (271, 275)  (276, 278)  (279, 282)  (283, 287)  (288, 296)   \n",
       "\n",
       "                        54          55          56          57          58  \\\n",
       "token                   is         the    Basilica          of         the   \n",
       "token_id              1110        1103       19349        1104        1103   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (297, 299)  (300, 303)  (304, 312)  (313, 315)  (316, 319)   \n",
       "\n",
       "                        59          60          61           62          63  \\\n",
       "token               Sacred       Heart           .  Immediately      behind   \n",
       "token_id             11373        4641         119        13301        1481   \n",
       "token_type_id            1           1           1            1           1   \n",
       "attention_mask           1           1           1            1           1   \n",
       "offset_mapping  (320, 326)  (327, 332)  (332, 333)   (334, 345)  (346, 352)   \n",
       "\n",
       "                        64          65          66          67          68  \\\n",
       "token                  the           b       ##asi      ##lica          is   \n",
       "token_id              1103         171       17506        9538        1110   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (353, 356)  (357, 358)  (358, 361)  (361, 365)  (366, 368)   \n",
       "\n",
       "                        69          70          71          72          73  \\\n",
       "token                  the           G       ##rot        ##to           ,   \n",
       "token_id              1103         144       10595        2430         117   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (369, 372)  (373, 374)  (374, 377)  (377, 379)  (379, 380)   \n",
       "\n",
       "                        74          75          76          77          78  \\\n",
       "token                    a      Marian       place          of      prayer   \n",
       "token_id               170       14789        1282        1104        8070   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (381, 382)  (383, 389)  (390, 395)  (396, 398)  (399, 405)   \n",
       "\n",
       "                        79          80          81          82          83  \\\n",
       "token                  and  reflection           .          It          is   \n",
       "token_id              1105        9284         119        1135        1110   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (406, 409)  (410, 420)  (420, 421)  (422, 424)  (425, 427)   \n",
       "\n",
       "                        84          85          86          87          88  \\\n",
       "token                    a     replica          of         the           g   \n",
       "token_id               170       16498        1104        1103         176   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (428, 429)  (430, 437)  (438, 440)  (441, 444)  (445, 446)   \n",
       "\n",
       "                        89          90          91          92          93  \\\n",
       "token                ##rot        ##to          at         Lou      ##rdes   \n",
       "token_id             10595        2430        1120       10111       20500   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (446, 449)  (449, 451)  (452, 454)  (455, 458)  (458, 462)   \n",
       "\n",
       "                        94          95          96          97          98  \\\n",
       "token                    ,      France       where         the      Virgin   \n",
       "token_id               117        1699        1187        1103        6567   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (462, 463)  (464, 470)  (471, 476)  (477, 480)  (481, 487)   \n",
       "\n",
       "                    99  \n",
       "token            [SEP]  \n",
       "token_id           102  \n",
       "token_type_id        1  \n",
       "attention_mask       1  \n",
       "offset_mapping  (0, 0)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- sentence 2: [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
      "- length: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>To</td>\n",
       "      <td>whom</td>\n",
       "      <td>did</td>\n",
       "      <td>the</td>\n",
       "      <td>Virgin</td>\n",
       "      <td>Mary</td>\n",
       "      <td>allegedly</td>\n",
       "      <td>appear</td>\n",
       "      <td>in</td>\n",
       "      <td>1858</td>\n",
       "      <td>in</td>\n",
       "      <td>Lou</td>\n",
       "      <td>##rdes</td>\n",
       "      <td>France</td>\n",
       "      <td>?</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>Next</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>Main</td>\n",
       "      <td>Building</td>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "      <td>Basilica</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>Sacred</td>\n",
       "      <td>Heart</td>\n",
       "      <td>.</td>\n",
       "      <td>Immediately</td>\n",
       "      <td>behind</td>\n",
       "      <td>the</td>\n",
       "      <td>b</td>\n",
       "      <td>##asi</td>\n",
       "      <td>##lica</td>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "      <td>G</td>\n",
       "      <td>##rot</td>\n",
       "      <td>##to</td>\n",
       "      <td>,</td>\n",
       "      <td>a</td>\n",
       "      <td>Marian</td>\n",
       "      <td>place</td>\n",
       "      <td>of</td>\n",
       "      <td>prayer</td>\n",
       "      <td>and</td>\n",
       "      <td>reflection</td>\n",
       "      <td>.</td>\n",
       "      <td>It</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>replica</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>g</td>\n",
       "      <td>##rot</td>\n",
       "      <td>##to</td>\n",
       "      <td>at</td>\n",
       "      <td>Lou</td>\n",
       "      <td>##rdes</td>\n",
       "      <td>,</td>\n",
       "      <td>France</td>\n",
       "      <td>where</td>\n",
       "      <td>the</td>\n",
       "      <td>Virgin</td>\n",
       "      <td>Mary</td>\n",
       "      <td>reputed</td>\n",
       "      <td>##ly</td>\n",
       "      <td>appeared</td>\n",
       "      <td>to</td>\n",
       "      <td>Saint</td>\n",
       "      <td>Bern</td>\n",
       "      <td>##ade</td>\n",
       "      <td>##tte</td>\n",
       "      <td>So</td>\n",
       "      <td>##ubi</td>\n",
       "      <td>##rous</td>\n",
       "      <td>in</td>\n",
       "      <td>1858</td>\n",
       "      <td>.</td>\n",
       "      <td>At</td>\n",
       "      <td>the</td>\n",
       "      <td>end</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>main</td>\n",
       "      <td>drive</td>\n",
       "      <td>(</td>\n",
       "      <td>and</td>\n",
       "      <td>in</td>\n",
       "      <td>a</td>\n",
       "      <td>direct</td>\n",
       "      <td>line</td>\n",
       "      <td>that</td>\n",
       "      <td>connects</td>\n",
       "      <td>through</td>\n",
       "      <td>3</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <td>101</td>\n",
       "      <td>1706</td>\n",
       "      <td>2292</td>\n",
       "      <td>1225</td>\n",
       "      <td>1103</td>\n",
       "      <td>6567</td>\n",
       "      <td>2090</td>\n",
       "      <td>9273</td>\n",
       "      <td>2845</td>\n",
       "      <td>1107</td>\n",
       "      <td>8109</td>\n",
       "      <td>1107</td>\n",
       "      <td>10111</td>\n",
       "      <td>20500</td>\n",
       "      <td>1699</td>\n",
       "      <td>136</td>\n",
       "      <td>102</td>\n",
       "      <td>5893</td>\n",
       "      <td>1106</td>\n",
       "      <td>1103</td>\n",
       "      <td>4304</td>\n",
       "      <td>4334</td>\n",
       "      <td>1110</td>\n",
       "      <td>1103</td>\n",
       "      <td>19349</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>11373</td>\n",
       "      <td>4641</td>\n",
       "      <td>119</td>\n",
       "      <td>13301</td>\n",
       "      <td>1481</td>\n",
       "      <td>1103</td>\n",
       "      <td>171</td>\n",
       "      <td>17506</td>\n",
       "      <td>9538</td>\n",
       "      <td>1110</td>\n",
       "      <td>1103</td>\n",
       "      <td>144</td>\n",
       "      <td>10595</td>\n",
       "      <td>2430</td>\n",
       "      <td>117</td>\n",
       "      <td>170</td>\n",
       "      <td>14789</td>\n",
       "      <td>1282</td>\n",
       "      <td>1104</td>\n",
       "      <td>8070</td>\n",
       "      <td>1105</td>\n",
       "      <td>9284</td>\n",
       "      <td>119</td>\n",
       "      <td>1135</td>\n",
       "      <td>1110</td>\n",
       "      <td>170</td>\n",
       "      <td>16498</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>176</td>\n",
       "      <td>10595</td>\n",
       "      <td>2430</td>\n",
       "      <td>1120</td>\n",
       "      <td>10111</td>\n",
       "      <td>20500</td>\n",
       "      <td>117</td>\n",
       "      <td>1699</td>\n",
       "      <td>1187</td>\n",
       "      <td>1103</td>\n",
       "      <td>6567</td>\n",
       "      <td>2090</td>\n",
       "      <td>25153</td>\n",
       "      <td>1193</td>\n",
       "      <td>1691</td>\n",
       "      <td>1106</td>\n",
       "      <td>2216</td>\n",
       "      <td>17666</td>\n",
       "      <td>6397</td>\n",
       "      <td>3786</td>\n",
       "      <td>1573</td>\n",
       "      <td>25422</td>\n",
       "      <td>13149</td>\n",
       "      <td>1107</td>\n",
       "      <td>8109</td>\n",
       "      <td>119</td>\n",
       "      <td>1335</td>\n",
       "      <td>1103</td>\n",
       "      <td>1322</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>1514</td>\n",
       "      <td>2797</td>\n",
       "      <td>113</td>\n",
       "      <td>1105</td>\n",
       "      <td>1107</td>\n",
       "      <td>170</td>\n",
       "      <td>2904</td>\n",
       "      <td>1413</td>\n",
       "      <td>1115</td>\n",
       "      <td>8200</td>\n",
       "      <td>1194</td>\n",
       "      <td>124</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_type_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offset_mapping</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(3, 7)</td>\n",
       "      <td>(8, 11)</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>(16, 22)</td>\n",
       "      <td>(23, 27)</td>\n",
       "      <td>(28, 37)</td>\n",
       "      <td>(38, 44)</td>\n",
       "      <td>(45, 47)</td>\n",
       "      <td>(48, 52)</td>\n",
       "      <td>(53, 55)</td>\n",
       "      <td>(56, 59)</td>\n",
       "      <td>(59, 63)</td>\n",
       "      <td>(64, 70)</td>\n",
       "      <td>(70, 71)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(271, 275)</td>\n",
       "      <td>(276, 278)</td>\n",
       "      <td>(279, 282)</td>\n",
       "      <td>(283, 287)</td>\n",
       "      <td>(288, 296)</td>\n",
       "      <td>(297, 299)</td>\n",
       "      <td>(300, 303)</td>\n",
       "      <td>(304, 312)</td>\n",
       "      <td>(313, 315)</td>\n",
       "      <td>(316, 319)</td>\n",
       "      <td>(320, 326)</td>\n",
       "      <td>(327, 332)</td>\n",
       "      <td>(332, 333)</td>\n",
       "      <td>(334, 345)</td>\n",
       "      <td>(346, 352)</td>\n",
       "      <td>(353, 356)</td>\n",
       "      <td>(357, 358)</td>\n",
       "      <td>(358, 361)</td>\n",
       "      <td>(361, 365)</td>\n",
       "      <td>(366, 368)</td>\n",
       "      <td>(369, 372)</td>\n",
       "      <td>(373, 374)</td>\n",
       "      <td>(374, 377)</td>\n",
       "      <td>(377, 379)</td>\n",
       "      <td>(379, 380)</td>\n",
       "      <td>(381, 382)</td>\n",
       "      <td>(383, 389)</td>\n",
       "      <td>(390, 395)</td>\n",
       "      <td>(396, 398)</td>\n",
       "      <td>(399, 405)</td>\n",
       "      <td>(406, 409)</td>\n",
       "      <td>(410, 420)</td>\n",
       "      <td>(420, 421)</td>\n",
       "      <td>(422, 424)</td>\n",
       "      <td>(425, 427)</td>\n",
       "      <td>(428, 429)</td>\n",
       "      <td>(430, 437)</td>\n",
       "      <td>(438, 440)</td>\n",
       "      <td>(441, 444)</td>\n",
       "      <td>(445, 446)</td>\n",
       "      <td>(446, 449)</td>\n",
       "      <td>(449, 451)</td>\n",
       "      <td>(452, 454)</td>\n",
       "      <td>(455, 458)</td>\n",
       "      <td>(458, 462)</td>\n",
       "      <td>(462, 463)</td>\n",
       "      <td>(464, 470)</td>\n",
       "      <td>(471, 476)</td>\n",
       "      <td>(477, 480)</td>\n",
       "      <td>(481, 487)</td>\n",
       "      <td>(488, 492)</td>\n",
       "      <td>(493, 500)</td>\n",
       "      <td>(500, 502)</td>\n",
       "      <td>(503, 511)</td>\n",
       "      <td>(512, 514)</td>\n",
       "      <td>(515, 520)</td>\n",
       "      <td>(521, 525)</td>\n",
       "      <td>(525, 528)</td>\n",
       "      <td>(528, 531)</td>\n",
       "      <td>(532, 534)</td>\n",
       "      <td>(534, 537)</td>\n",
       "      <td>(537, 541)</td>\n",
       "      <td>(542, 544)</td>\n",
       "      <td>(545, 549)</td>\n",
       "      <td>(549, 550)</td>\n",
       "      <td>(551, 553)</td>\n",
       "      <td>(554, 557)</td>\n",
       "      <td>(558, 561)</td>\n",
       "      <td>(562, 564)</td>\n",
       "      <td>(565, 568)</td>\n",
       "      <td>(569, 573)</td>\n",
       "      <td>(574, 579)</td>\n",
       "      <td>(580, 581)</td>\n",
       "      <td>(581, 584)</td>\n",
       "      <td>(585, 587)</td>\n",
       "      <td>(588, 589)</td>\n",
       "      <td>(590, 596)</td>\n",
       "      <td>(597, 601)</td>\n",
       "      <td>(602, 606)</td>\n",
       "      <td>(607, 615)</td>\n",
       "      <td>(616, 623)</td>\n",
       "      <td>(624, 625)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1       2        3         4         5         6   \\\n",
       "token            [CLS]      To    whom      did       the    Virgin      Mary   \n",
       "token_id           101    1706    2292     1225      1103      6567      2090   \n",
       "token_type_id        0       0       0        0         0         0         0   \n",
       "attention_mask       1       1       1        1         1         1         1   \n",
       "offset_mapping  (0, 0)  (0, 2)  (3, 7)  (8, 11)  (12, 15)  (16, 22)  (23, 27)   \n",
       "\n",
       "                       7         8         9         10        11        12  \\\n",
       "token           allegedly    appear        in      1858        in       Lou   \n",
       "token_id             9273      2845      1107      8109      1107     10111   \n",
       "token_type_id           0         0         0         0         0         0   \n",
       "attention_mask          1         1         1         1         1         1   \n",
       "offset_mapping   (28, 37)  (38, 44)  (45, 47)  (48, 52)  (53, 55)  (56, 59)   \n",
       "\n",
       "                      13        14        15      16          17          18  \\\n",
       "token             ##rdes    France         ?   [SEP]        Next          to   \n",
       "token_id           20500      1699       136     102        5893        1106   \n",
       "token_type_id          0         0         0       0           1           1   \n",
       "attention_mask         1         1         1       1           1           1   \n",
       "offset_mapping  (59, 63)  (64, 70)  (70, 71)  (0, 0)  (271, 275)  (276, 278)   \n",
       "\n",
       "                        19          20          21          22          23  \\\n",
       "token                  the        Main    Building          is         the   \n",
       "token_id              1103        4304        4334        1110        1103   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (279, 282)  (283, 287)  (288, 296)  (297, 299)  (300, 303)   \n",
       "\n",
       "                        24          25          26          27          28  \\\n",
       "token             Basilica          of         the      Sacred       Heart   \n",
       "token_id             19349        1104        1103       11373        4641   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (304, 312)  (313, 315)  (316, 319)  (320, 326)  (327, 332)   \n",
       "\n",
       "                        29           30          31          32          33  \\\n",
       "token                    .  Immediately      behind         the           b   \n",
       "token_id               119        13301        1481        1103         171   \n",
       "token_type_id            1            1           1           1           1   \n",
       "attention_mask           1            1           1           1           1   \n",
       "offset_mapping  (332, 333)   (334, 345)  (346, 352)  (353, 356)  (357, 358)   \n",
       "\n",
       "                        34          35          36          37          38  \\\n",
       "token                ##asi      ##lica          is         the           G   \n",
       "token_id             17506        9538        1110        1103         144   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (358, 361)  (361, 365)  (366, 368)  (369, 372)  (373, 374)   \n",
       "\n",
       "                        39          40          41          42          43  \\\n",
       "token                ##rot        ##to           ,           a      Marian   \n",
       "token_id             10595        2430         117         170       14789   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (374, 377)  (377, 379)  (379, 380)  (381, 382)  (383, 389)   \n",
       "\n",
       "                        44          45          46          47          48  \\\n",
       "token                place          of      prayer         and  reflection   \n",
       "token_id              1282        1104        8070        1105        9284   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (390, 395)  (396, 398)  (399, 405)  (406, 409)  (410, 420)   \n",
       "\n",
       "                        49          50          51          52          53  \\\n",
       "token                    .          It          is           a     replica   \n",
       "token_id               119        1135        1110         170       16498   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (420, 421)  (422, 424)  (425, 427)  (428, 429)  (430, 437)   \n",
       "\n",
       "                        54          55          56          57          58  \\\n",
       "token                   of         the           g       ##rot        ##to   \n",
       "token_id              1104        1103         176       10595        2430   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (438, 440)  (441, 444)  (445, 446)  (446, 449)  (449, 451)   \n",
       "\n",
       "                        59          60          61          62          63  \\\n",
       "token                   at         Lou      ##rdes           ,      France   \n",
       "token_id              1120       10111       20500         117        1699   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (452, 454)  (455, 458)  (458, 462)  (462, 463)  (464, 470)   \n",
       "\n",
       "                        64          65          66          67          68  \\\n",
       "token                where         the      Virgin        Mary     reputed   \n",
       "token_id              1187        1103        6567        2090       25153   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (471, 476)  (477, 480)  (481, 487)  (488, 492)  (493, 500)   \n",
       "\n",
       "                        69          70          71          72          73  \\\n",
       "token                 ##ly    appeared          to       Saint        Bern   \n",
       "token_id              1193        1691        1106        2216       17666   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (500, 502)  (503, 511)  (512, 514)  (515, 520)  (521, 525)   \n",
       "\n",
       "                        74          75          76          77          78  \\\n",
       "token                ##ade       ##tte          So       ##ubi      ##rous   \n",
       "token_id              6397        3786        1573       25422       13149   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (525, 528)  (528, 531)  (532, 534)  (534, 537)  (537, 541)   \n",
       "\n",
       "                        79          80          81          82          83  \\\n",
       "token                   in        1858           .          At         the   \n",
       "token_id              1107        8109         119        1335        1103   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (542, 544)  (545, 549)  (549, 550)  (551, 553)  (554, 557)   \n",
       "\n",
       "                        84          85          86          87          88  \\\n",
       "token                  end          of         the        main       drive   \n",
       "token_id              1322        1104        1103        1514        2797   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (558, 561)  (562, 564)  (565, 568)  (569, 573)  (574, 579)   \n",
       "\n",
       "                        89          90          91          92          93  \\\n",
       "token                    (         and          in           a      direct   \n",
       "token_id               113        1105        1107         170        2904   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (580, 581)  (581, 584)  (585, 587)  (588, 589)  (590, 596)   \n",
       "\n",
       "                        94          95          96          97          98  \\\n",
       "token                 line        that    connects     through           3   \n",
       "token_id              1413        1115        8200        1194         124   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (597, 601)  (602, 606)  (607, 615)  (616, 623)  (624, 625)   \n",
       "\n",
       "                    99  \n",
       "token            [SEP]  \n",
       "token_id           102  \n",
       "token_type_id        1  \n",
       "attention_mask       1  \n",
       "offset_mapping  (0, 0)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- sentence 3: [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
      "- length: 85\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>To</td>\n",
       "      <td>whom</td>\n",
       "      <td>did</td>\n",
       "      <td>the</td>\n",
       "      <td>Virgin</td>\n",
       "      <td>Mary</td>\n",
       "      <td>allegedly</td>\n",
       "      <td>appear</td>\n",
       "      <td>in</td>\n",
       "      <td>1858</td>\n",
       "      <td>in</td>\n",
       "      <td>Lou</td>\n",
       "      <td>##rdes</td>\n",
       "      <td>France</td>\n",
       "      <td>?</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>.</td>\n",
       "      <td>It</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>replica</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>g</td>\n",
       "      <td>##rot</td>\n",
       "      <td>##to</td>\n",
       "      <td>at</td>\n",
       "      <td>Lou</td>\n",
       "      <td>##rdes</td>\n",
       "      <td>,</td>\n",
       "      <td>France</td>\n",
       "      <td>where</td>\n",
       "      <td>the</td>\n",
       "      <td>Virgin</td>\n",
       "      <td>Mary</td>\n",
       "      <td>reputed</td>\n",
       "      <td>##ly</td>\n",
       "      <td>appeared</td>\n",
       "      <td>to</td>\n",
       "      <td>Saint</td>\n",
       "      <td>Bern</td>\n",
       "      <td>##ade</td>\n",
       "      <td>##tte</td>\n",
       "      <td>So</td>\n",
       "      <td>##ubi</td>\n",
       "      <td>##rous</td>\n",
       "      <td>in</td>\n",
       "      <td>1858</td>\n",
       "      <td>.</td>\n",
       "      <td>At</td>\n",
       "      <td>the</td>\n",
       "      <td>end</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>main</td>\n",
       "      <td>drive</td>\n",
       "      <td>(</td>\n",
       "      <td>and</td>\n",
       "      <td>in</td>\n",
       "      <td>a</td>\n",
       "      <td>direct</td>\n",
       "      <td>line</td>\n",
       "      <td>that</td>\n",
       "      <td>connects</td>\n",
       "      <td>through</td>\n",
       "      <td>3</td>\n",
       "      <td>statues</td>\n",
       "      <td>and</td>\n",
       "      <td>the</td>\n",
       "      <td>Gold</td>\n",
       "      <td>Dome</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>simple</td>\n",
       "      <td>,</td>\n",
       "      <td>modern</td>\n",
       "      <td>stone</td>\n",
       "      <td>statue</td>\n",
       "      <td>of</td>\n",
       "      <td>Mary</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <td>101</td>\n",
       "      <td>1706</td>\n",
       "      <td>2292</td>\n",
       "      <td>1225</td>\n",
       "      <td>1103</td>\n",
       "      <td>6567</td>\n",
       "      <td>2090</td>\n",
       "      <td>9273</td>\n",
       "      <td>2845</td>\n",
       "      <td>1107</td>\n",
       "      <td>8109</td>\n",
       "      <td>1107</td>\n",
       "      <td>10111</td>\n",
       "      <td>20500</td>\n",
       "      <td>1699</td>\n",
       "      <td>136</td>\n",
       "      <td>102</td>\n",
       "      <td>119</td>\n",
       "      <td>1135</td>\n",
       "      <td>1110</td>\n",
       "      <td>170</td>\n",
       "      <td>16498</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>176</td>\n",
       "      <td>10595</td>\n",
       "      <td>2430</td>\n",
       "      <td>1120</td>\n",
       "      <td>10111</td>\n",
       "      <td>20500</td>\n",
       "      <td>117</td>\n",
       "      <td>1699</td>\n",
       "      <td>1187</td>\n",
       "      <td>1103</td>\n",
       "      <td>6567</td>\n",
       "      <td>2090</td>\n",
       "      <td>25153</td>\n",
       "      <td>1193</td>\n",
       "      <td>1691</td>\n",
       "      <td>1106</td>\n",
       "      <td>2216</td>\n",
       "      <td>17666</td>\n",
       "      <td>6397</td>\n",
       "      <td>3786</td>\n",
       "      <td>1573</td>\n",
       "      <td>25422</td>\n",
       "      <td>13149</td>\n",
       "      <td>1107</td>\n",
       "      <td>8109</td>\n",
       "      <td>119</td>\n",
       "      <td>1335</td>\n",
       "      <td>1103</td>\n",
       "      <td>1322</td>\n",
       "      <td>1104</td>\n",
       "      <td>1103</td>\n",
       "      <td>1514</td>\n",
       "      <td>2797</td>\n",
       "      <td>113</td>\n",
       "      <td>1105</td>\n",
       "      <td>1107</td>\n",
       "      <td>170</td>\n",
       "      <td>2904</td>\n",
       "      <td>1413</td>\n",
       "      <td>1115</td>\n",
       "      <td>8200</td>\n",
       "      <td>1194</td>\n",
       "      <td>124</td>\n",
       "      <td>11739</td>\n",
       "      <td>1105</td>\n",
       "      <td>1103</td>\n",
       "      <td>3487</td>\n",
       "      <td>17917</td>\n",
       "      <td>114</td>\n",
       "      <td>117</td>\n",
       "      <td>1110</td>\n",
       "      <td>170</td>\n",
       "      <td>3014</td>\n",
       "      <td>117</td>\n",
       "      <td>2030</td>\n",
       "      <td>2576</td>\n",
       "      <td>5921</td>\n",
       "      <td>1104</td>\n",
       "      <td>2090</td>\n",
       "      <td>119</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_type_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offset_mapping</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(3, 7)</td>\n",
       "      <td>(8, 11)</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>(16, 22)</td>\n",
       "      <td>(23, 27)</td>\n",
       "      <td>(28, 37)</td>\n",
       "      <td>(38, 44)</td>\n",
       "      <td>(45, 47)</td>\n",
       "      <td>(48, 52)</td>\n",
       "      <td>(53, 55)</td>\n",
       "      <td>(56, 59)</td>\n",
       "      <td>(59, 63)</td>\n",
       "      <td>(64, 70)</td>\n",
       "      <td>(70, 71)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(420, 421)</td>\n",
       "      <td>(422, 424)</td>\n",
       "      <td>(425, 427)</td>\n",
       "      <td>(428, 429)</td>\n",
       "      <td>(430, 437)</td>\n",
       "      <td>(438, 440)</td>\n",
       "      <td>(441, 444)</td>\n",
       "      <td>(445, 446)</td>\n",
       "      <td>(446, 449)</td>\n",
       "      <td>(449, 451)</td>\n",
       "      <td>(452, 454)</td>\n",
       "      <td>(455, 458)</td>\n",
       "      <td>(458, 462)</td>\n",
       "      <td>(462, 463)</td>\n",
       "      <td>(464, 470)</td>\n",
       "      <td>(471, 476)</td>\n",
       "      <td>(477, 480)</td>\n",
       "      <td>(481, 487)</td>\n",
       "      <td>(488, 492)</td>\n",
       "      <td>(493, 500)</td>\n",
       "      <td>(500, 502)</td>\n",
       "      <td>(503, 511)</td>\n",
       "      <td>(512, 514)</td>\n",
       "      <td>(515, 520)</td>\n",
       "      <td>(521, 525)</td>\n",
       "      <td>(525, 528)</td>\n",
       "      <td>(528, 531)</td>\n",
       "      <td>(532, 534)</td>\n",
       "      <td>(534, 537)</td>\n",
       "      <td>(537, 541)</td>\n",
       "      <td>(542, 544)</td>\n",
       "      <td>(545, 549)</td>\n",
       "      <td>(549, 550)</td>\n",
       "      <td>(551, 553)</td>\n",
       "      <td>(554, 557)</td>\n",
       "      <td>(558, 561)</td>\n",
       "      <td>(562, 564)</td>\n",
       "      <td>(565, 568)</td>\n",
       "      <td>(569, 573)</td>\n",
       "      <td>(574, 579)</td>\n",
       "      <td>(580, 581)</td>\n",
       "      <td>(581, 584)</td>\n",
       "      <td>(585, 587)</td>\n",
       "      <td>(588, 589)</td>\n",
       "      <td>(590, 596)</td>\n",
       "      <td>(597, 601)</td>\n",
       "      <td>(602, 606)</td>\n",
       "      <td>(607, 615)</td>\n",
       "      <td>(616, 623)</td>\n",
       "      <td>(624, 625)</td>\n",
       "      <td>(626, 633)</td>\n",
       "      <td>(634, 637)</td>\n",
       "      <td>(638, 641)</td>\n",
       "      <td>(642, 646)</td>\n",
       "      <td>(647, 651)</td>\n",
       "      <td>(651, 652)</td>\n",
       "      <td>(652, 653)</td>\n",
       "      <td>(654, 656)</td>\n",
       "      <td>(657, 658)</td>\n",
       "      <td>(659, 665)</td>\n",
       "      <td>(665, 666)</td>\n",
       "      <td>(667, 673)</td>\n",
       "      <td>(674, 679)</td>\n",
       "      <td>(680, 686)</td>\n",
       "      <td>(687, 689)</td>\n",
       "      <td>(690, 694)</td>\n",
       "      <td>(694, 695)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1       2        3         4         5         6   \\\n",
       "token            [CLS]      To    whom      did       the    Virgin      Mary   \n",
       "token_id           101    1706    2292     1225      1103      6567      2090   \n",
       "token_type_id        0       0       0        0         0         0         0   \n",
       "attention_mask       1       1       1        1         1         1         1   \n",
       "offset_mapping  (0, 0)  (0, 2)  (3, 7)  (8, 11)  (12, 15)  (16, 22)  (23, 27)   \n",
       "\n",
       "                       7         8         9         10        11        12  \\\n",
       "token           allegedly    appear        in      1858        in       Lou   \n",
       "token_id             9273      2845      1107      8109      1107     10111   \n",
       "token_type_id           0         0         0         0         0         0   \n",
       "attention_mask          1         1         1         1         1         1   \n",
       "offset_mapping   (28, 37)  (38, 44)  (45, 47)  (48, 52)  (53, 55)  (56, 59)   \n",
       "\n",
       "                      13        14        15      16          17          18  \\\n",
       "token             ##rdes    France         ?   [SEP]           .          It   \n",
       "token_id           20500      1699       136     102         119        1135   \n",
       "token_type_id          0         0         0       0           1           1   \n",
       "attention_mask         1         1         1       1           1           1   \n",
       "offset_mapping  (59, 63)  (64, 70)  (70, 71)  (0, 0)  (420, 421)  (422, 424)   \n",
       "\n",
       "                        19          20          21          22          23  \\\n",
       "token                   is           a     replica          of         the   \n",
       "token_id              1110         170       16498        1104        1103   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (425, 427)  (428, 429)  (430, 437)  (438, 440)  (441, 444)   \n",
       "\n",
       "                        24          25          26          27          28  \\\n",
       "token                    g       ##rot        ##to          at         Lou   \n",
       "token_id               176       10595        2430        1120       10111   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (445, 446)  (446, 449)  (449, 451)  (452, 454)  (455, 458)   \n",
       "\n",
       "                        29          30          31          32          33  \\\n",
       "token               ##rdes           ,      France       where         the   \n",
       "token_id             20500         117        1699        1187        1103   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (458, 462)  (462, 463)  (464, 470)  (471, 476)  (477, 480)   \n",
       "\n",
       "                        34          35          36          37          38  \\\n",
       "token               Virgin        Mary     reputed        ##ly    appeared   \n",
       "token_id              6567        2090       25153        1193        1691   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (481, 487)  (488, 492)  (493, 500)  (500, 502)  (503, 511)   \n",
       "\n",
       "                        39          40          41          42          43  \\\n",
       "token                   to       Saint        Bern       ##ade       ##tte   \n",
       "token_id              1106        2216       17666        6397        3786   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (512, 514)  (515, 520)  (521, 525)  (525, 528)  (528, 531)   \n",
       "\n",
       "                        44          45          46          47          48  \\\n",
       "token                   So       ##ubi      ##rous          in        1858   \n",
       "token_id              1573       25422       13149        1107        8109   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (532, 534)  (534, 537)  (537, 541)  (542, 544)  (545, 549)   \n",
       "\n",
       "                        49          50          51          52          53  \\\n",
       "token                    .          At         the         end          of   \n",
       "token_id               119        1335        1103        1322        1104   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (549, 550)  (551, 553)  (554, 557)  (558, 561)  (562, 564)   \n",
       "\n",
       "                        54          55          56          57          58  \\\n",
       "token                  the        main       drive           (         and   \n",
       "token_id              1103        1514        2797         113        1105   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (565, 568)  (569, 573)  (574, 579)  (580, 581)  (581, 584)   \n",
       "\n",
       "                        59          60          61          62          63  \\\n",
       "token                   in           a      direct        line        that   \n",
       "token_id              1107         170        2904        1413        1115   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (585, 587)  (588, 589)  (590, 596)  (597, 601)  (602, 606)   \n",
       "\n",
       "                        64          65          66          67          68  \\\n",
       "token             connects     through           3     statues         and   \n",
       "token_id              8200        1194         124       11739        1105   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (607, 615)  (616, 623)  (624, 625)  (626, 633)  (634, 637)   \n",
       "\n",
       "                        69          70          71          72          73  \\\n",
       "token                  the        Gold        Dome           )           ,   \n",
       "token_id              1103        3487       17917         114         117   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (638, 641)  (642, 646)  (647, 651)  (651, 652)  (652, 653)   \n",
       "\n",
       "                        74          75          76          77          78  \\\n",
       "token                   is           a      simple           ,      modern   \n",
       "token_id              1110         170        3014         117        2030   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (654, 656)  (657, 658)  (659, 665)  (665, 666)  (667, 673)   \n",
       "\n",
       "                        79          80          81          82          83  \\\n",
       "token                stone      statue          of        Mary           .   \n",
       "token_id              2576        5921        1104        2090         119   \n",
       "token_type_id            1           1           1           1           1   \n",
       "attention_mask           1           1           1           1           1   \n",
       "offset_mapping  (674, 679)  (680, 686)  (687, 689)  (690, 694)  (694, 695)   \n",
       "\n",
       "                    84  \n",
       "token            [SEP]  \n",
       "token_id           102  \n",
       "token_type_id        1  \n",
       "attention_mask       1  \n",
       "offset_mapping  (0, 0)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "for idx_sent in range(n_sents):\n",
    "    print(f\"- sentence {idx_sent}:\", tokenizer.decode(inputs['input_ids'][idx_sent]))\n",
    "    print(\"- length:\", len(inputs['input_ids'][idx_sent]))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'token': [tokenizer.decode(token_id) for token_id in inputs['input_ids'][idx_sent]],\n",
    "        'token_id': inputs['input_ids'][idx_sent],\n",
    "        'token_type_id': inputs['token_type_ids'][idx_sent],\n",
    "        'attention_mask': inputs['attention_mask'][idx_sent],\n",
    "        'offset_mapping': inputs['offset_mapping'][idx_sent]\n",
    "    })\n",
    "    display(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68002078",
   "metadata": {},
   "source": [
    "1. `max_length=100`\n",
    "    - The number of tokens in a sentence.\n",
    "2. `stride`\n",
    "    - The number of overlapping tokens.\n",
    "3. `return_overflowing_tokens`\n",
    "    - Let the tokenizer know we want the overflowing tokens\n",
    "4. `offset_mapping`\n",
    "    - `(0, 0)`: Special token(`[CLS]`, `[SEP]`)\n",
    "    - `(s, e)`: `question[s:e]` or `context[s:e]`\n",
    "5. `token_type_ids`\n",
    "    - Whether the token is special token or question or context\n",
    "    - **Since those do not necessarily exist for all models (DistilBERT does not require them, for instance), we‚Äôll instead use the `sequence_ids()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da6072aa49270ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:02:13.023167400Z",
     "start_time": "2023-12-20T06:02:13.021601300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 examples gave 19 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][2:6][\"question\"],\n",
    "    raw_datasets[\"train\"][2:6][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "answers = raw_datasets['train'][2:6]['answers']\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b3bd165ae3444381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T06:05:19.747525400Z",
     "start_time": "2023-12-20T06:05:19.738236100Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ['the Main Building'], 'answer_start': [279]},\n",
       " {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]},\n",
       " {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]},\n",
       " {'text': ['September 1876'], 'answer_start': [248]}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833bfe0",
   "metadata": {},
   "source": [
    "- Generate `(start_position, end_position)` tuple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7a4e9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_positions = []\n",
    "end_positions   = []\n",
    "\n",
    "for idx_feat, offset in enumerate(inputs['offset_mapping']):\n",
    "    idx_sample = inputs['overflow_to_sample_mapping'][idx_feat]  # idx of answer\n",
    "    answer     = answers[idx_sample]\n",
    "    start_char = answer['answer_start'][0]  # offset\n",
    "    end_char   = start_char + len(answer['text'][0])\n",
    "    sequence_ids = inputs.sequence_ids(idx_feat)  # None: [CLS] or [SEP], 0: question, 1: context \n",
    "    \n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx  # sequence_ids[context_start] == 1\n",
    "    \n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1    # sequence_ids[context_end] == 1\n",
    "    \n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] <= start_char and end_char <= offset[context_end][1]:\n",
    "        idx = context_start\n",
    "        while offset[idx][0] < start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx)\n",
    "\n",
    "        while offset[idx][1] < end_char:\n",
    "            idx += 1\n",
    "        end_positions.append(idx)    \n",
    "    else:\n",
    "        start_positions.append(0), end_positions.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a6460c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n",
       " [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda2e8b",
   "metadata": {},
   "source": [
    "- (83, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2668d64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: the Main Building, labels give: the Main Building\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "sample_idx = inputs['overflow_to_sample_mapping'][idx]\n",
    "answer = answers[sample_idx]['text'][0]\n",
    "\n",
    "start, end = start_positions[idx], end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs['input_ids'][idx][start:end+1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10f1ab",
   "metadata": {},
   "source": [
    "- (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "552d1601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
    "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ea810e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e94f6264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 88729)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets['train'].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets['train'].column_names\n",
    ")\n",
    "len(raw_datasets['train']), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6addf",
   "metadata": {},
   "source": [
    "## Processing the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a0e36230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples['question']]\n",
    "    contexts  = examples['context']\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=max_length,\n",
    "        truncation='only_second',\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    sample_map  = inputs.pop('overflow_to_sample_mapping')\n",
    "    example_ids = []\n",
    "    \n",
    "    for i in range(len(inputs['input_ids'])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples['id'][sample_idx])\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)  # None: [CLS] or [SEP], 0: question, 1: context\n",
    "        offset = inputs['offset_mapping'][i]\n",
    "        inputs['offset_mapping'][i] = [o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)]  # use only context offset\n",
    "        \n",
    "    inputs['example_id'] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1e1f00f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaca976468b4d6eaa72a96175ce494a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets['validation'].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets['validation'].column_names\n",
    ")\n",
    "\n",
    "len(raw_datasets['validation']), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27395890",
   "metadata": {},
   "source": [
    "# Fine-tuning the model with the `Trainer` API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9d900",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3cefd",
   "metadata": {},
   "source": [
    "1. The hardest thing will be to write the `compute_metrics()` function\n",
    "2. Since we **padded** all the samples to the maximum length we set, there is **no data collator** to define\n",
    "3. Post-processing for **Question answering**\n",
    "    - We masked the start and end logits corresponding to tokens outside of the context.\n",
    "    - We then converted the start and end logits into probabilities using a softmax.\n",
    "    - We attributed a score to each (start_token, end_token) pair by taking the product of the corresponding two probabilities.\n",
    "    - We looked for the pair with the maximum score that yielded a valid answer (e.g., a start_token lower than end_token).\n",
    "4. Faster version\n",
    "    - We don‚Äôt need to compute actual scores (just the predicted answer) (skip the softmax step)\n",
    "    - We also won‚Äôt score all the possible (start_token, end_token) pairs, but only the ones corresponding to the highest n_best logits (with n_best=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519adce",
   "metadata": {},
   "source": [
    "- Sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "38545b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55824c1429f40e2b511ca019246d995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ebebc1cf7b47559e7367802b891d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7787bad014414efab36ca50c34dbb9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db09d259f4fc4d408c3d0a9a5fdf7092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94e2bf3843a4a8bb05a11b89c44a175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_eval_set = raw_datasets['validation'].select(range(100))\n",
    "trained_checkpoint = 'distilbert-base-cased-distilled-squad'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets['validation'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3eefd1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set_for_model = eval_set.remove_columns(['example_id', 'offset_mapping'])\n",
    "eval_set_for_model.set_format('torch')\n",
    "eval_set_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8eeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "90e525b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "device = torch.device('cuda')\n",
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "527e879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits   = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "efc02844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature['example_id']].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "964b0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example['id']\n",
    "    context = example['context']\n",
    "    answers = []\n",
    "    \n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit   = end_logits[feature_index]\n",
    "        offsets     = eval_set['offset_mapping'][feature_index]\n",
    "        \n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best-1 : -1].tolist()  # big n_best\n",
    "        end_indexes   = np.argsort(end_logit)[-1 : -n_best-1 : -1].tolist()  # big n_best\n",
    "        \n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                if (start_index > end_index) or (end_index - start_index + 1 > max_answer_length):\n",
    "                    continue\n",
    "                answers.append({\n",
    "                    'text': context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                    'logit_score': start_logit[start_index] + end_logit[end_index]\n",
    "                })\n",
    "    \n",
    "    best_answer = max(answers, key=lambda x: x['logit_score'])\n",
    "    predicted_answers.append({'id': example_id, 'prediction_text': best_answer['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7c4e1997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4db0acb8001400a502ed', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56be4db0acb8001400a502ee',\n",
       "  'prediction_text': \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\"},\n",
       " {'id': '56be4db0acb8001400a502ef', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56be4db0acb8001400a502f0', 'prediction_text': 'gold'},\n",
       " {'id': '56be8e613aeaaa14008c90d1', 'prediction_text': 'golden anniversary'},\n",
       " {'id': '56be8e613aeaaa14008c90d2', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56be8e613aeaaa14008c90d3',\n",
       "  'prediction_text': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference'},\n",
       " {'id': '56bea9923aeaaa14008c91b9', 'prediction_text': 'golden anniversary'},\n",
       " {'id': '56bea9923aeaaa14008c91ba',\n",
       "  'prediction_text': 'American Football Conference'},\n",
       " {'id': '56bea9923aeaaa14008c91bb', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56beace93aeaaa14008c91df', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56beace93aeaaa14008c91e0', 'prediction_text': \"Levi's Stadium\"},\n",
       " {'id': '56beace93aeaaa14008c91e1',\n",
       "  'prediction_text': 'Santa Clara, California'},\n",
       " {'id': '56beace93aeaaa14008c91e2', 'prediction_text': 'Super Bowl L'},\n",
       " {'id': '56beace93aeaaa14008c91e3', 'prediction_text': '2015'},\n",
       " {'id': '56bf10f43aeaaa14008c94fd', 'prediction_text': '2016'},\n",
       " {'id': '56bf10f43aeaaa14008c94fe',\n",
       "  'prediction_text': 'Santa Clara, California'},\n",
       " {'id': '56bf10f43aeaaa14008c94ff', 'prediction_text': \"Levi's Stadium\"},\n",
       " {'id': '56bf10f43aeaaa14008c9500', 'prediction_text': '24‚Äì10'},\n",
       " {'id': '56bf10f43aeaaa14008c9501', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56d20362e7d4791d009025e8', 'prediction_text': '2015'},\n",
       " {'id': '56d20362e7d4791d009025e9', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56d20362e7d4791d009025ea', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56d20362e7d4791d009025eb', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56d600e31c85041400946eae', 'prediction_text': '2015'},\n",
       " {'id': '56d600e31c85041400946eb0', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56d600e31c85041400946eb1', 'prediction_text': \"Levi's Stadium\"},\n",
       " {'id': '56d9895ddc89441400fdb50e', 'prediction_text': 'Super Bowl 50'},\n",
       " {'id': '56d9895ddc89441400fdb510', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4e1facb8001400a502f6', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56be4e1facb8001400a502f9', 'prediction_text': 'eight'},\n",
       " {'id': '56be4e1facb8001400a502fa', 'prediction_text': '1995'},\n",
       " {'id': '56beaa4a3aeaaa14008c91c2', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56beaa4a3aeaaa14008c91c3', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56bead5a3aeaaa14008c91e9', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56bead5a3aeaaa14008c91ea', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56bead5a3aeaaa14008c91eb', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56bead5a3aeaaa14008c91ec', 'prediction_text': 'four'},\n",
       " {'id': '56bead5a3aeaaa14008c91ed', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56bf159b3aeaaa14008c9507', 'prediction_text': '15‚Äì1'},\n",
       " {'id': '56bf159b3aeaaa14008c9508', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56bf159b3aeaaa14008c9509', 'prediction_text': '15‚Äì1 record'},\n",
       " {'id': '56bf159b3aeaaa14008c950a', 'prediction_text': 'four'},\n",
       " {'id': '56bf159b3aeaaa14008c950b', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56d2045de7d4791d009025f3', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d2045de7d4791d009025f4', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d2045de7d4791d009025f5', 'prediction_text': 'eight'},\n",
       " {'id': '56d2045de7d4791d009025f6', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d6017d1c85041400946ebe', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d6017d1c85041400946ec1', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d6017d1c85041400946ec2', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d98a59dc89441400fdb52a', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d98a59dc89441400fdb52b', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d98a59dc89441400fdb52e', 'prediction_text': '1995'},\n",
       " {'id': '56be4eafacb8001400a50302', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56be4eafacb8001400a50303', 'prediction_text': 'two'},\n",
       " {'id': '56be4eafacb8001400a50304', 'prediction_text': 'Broncos'},\n",
       " {'id': '56beab833aeaaa14008c91d2', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56beab833aeaaa14008c91d3', 'prediction_text': 'five'},\n",
       " {'id': '56beab833aeaaa14008c91d4', 'prediction_text': 'Newton'},\n",
       " {'id': '56beae423aeaaa14008c91f4', 'prediction_text': 'seven'},\n",
       " {'id': '56beae423aeaaa14008c91f5', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56beae423aeaaa14008c91f6', 'prediction_text': 'three'},\n",
       " {'id': '56beae423aeaaa14008c91f7', 'prediction_text': 'two'},\n",
       " {'id': '56bf17653aeaaa14008c9511', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56bf17653aeaaa14008c9513', 'prediction_text': 'linebacker'},\n",
       " {'id': '56bf17653aeaaa14008c9514', 'prediction_text': 'five'},\n",
       " {'id': '56bf17653aeaaa14008c9515', 'prediction_text': 'two'},\n",
       " {'id': '56d204ade7d4791d00902603', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d204ade7d4791d00902604', 'prediction_text': 'five'},\n",
       " {'id': '56d601e41c85041400946ece', 'prediction_text': 'seven'},\n",
       " {'id': '56d601e41c85041400946ecf', 'prediction_text': 'three'},\n",
       " {'id': '56d601e41c85041400946ed0', 'prediction_text': 'a fumble'},\n",
       " {'id': '56d601e41c85041400946ed1', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d601e41c85041400946ed2', 'prediction_text': 'linebacker'},\n",
       " {'id': '56d98b33dc89441400fdb53b', 'prediction_text': 'seven'},\n",
       " {'id': '56d98b33dc89441400fdb53c', 'prediction_text': 'three'},\n",
       " {'id': '56d98b33dc89441400fdb53d', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d98b33dc89441400fdb53e', 'prediction_text': 'five'},\n",
       " {'id': '56be5333acb8001400a5030a', 'prediction_text': 'CBS'},\n",
       " {'id': '56be5333acb8001400a5030b', 'prediction_text': '$5 million'},\n",
       " {'id': '56be5333acb8001400a5030c', 'prediction_text': 'Coldplay'},\n",
       " {'id': '56be5333acb8001400a5030d',\n",
       "  'prediction_text': 'Beyonc√© and Bruno Mars'},\n",
       " {'id': '56be5333acb8001400a5030e', 'prediction_text': 'Super Bowl 50'},\n",
       " {'id': '56beaf5e3aeaaa14008c91fd', 'prediction_text': 'CBS'},\n",
       " {'id': '56beaf5e3aeaaa14008c91fe', 'prediction_text': '$5 million'},\n",
       " {'id': '56beaf5e3aeaaa14008c91ff',\n",
       "  'prediction_text': 'Beyonc√© and Bruno Mars'},\n",
       " {'id': '56beaf5e3aeaaa14008c9200',\n",
       "  'prediction_text': 'Beyonc√© and Bruno Mars'},\n",
       " {'id': '56beaf5e3aeaaa14008c9201',\n",
       "  'prediction_text': 'Beyonc√© and Bruno Mars'},\n",
       " {'id': '56bf1ae93aeaaa14008c951b', 'prediction_text': 'CBS'},\n",
       " {'id': '56bf1ae93aeaaa14008c951c', 'prediction_text': '$5 million'},\n",
       " {'id': '56bf1ae93aeaaa14008c951e',\n",
       "  'prediction_text': 'Beyonc√© and Bruno Mars'},\n",
       " {'id': '56bf1ae93aeaaa14008c951f', 'prediction_text': 'third'},\n",
       " {'id': '56d2051ce7d4791d00902608', 'prediction_text': 'CBS'},\n",
       " {'id': '56d2051ce7d4791d00902609', 'prediction_text': '$5 million'},\n",
       " {'id': '56d2051ce7d4791d0090260a', 'prediction_text': 'Coldplay'},\n",
       " {'id': '56d2051ce7d4791d0090260b',\n",
       "  'prediction_text': 'Beyonc√© and Bruno Mars'},\n",
       " {'id': '56d602631c85041400946ed8', 'prediction_text': 'CBS'},\n",
       " {'id': '56d602631c85041400946eda',\n",
       "  'prediction_text': 'Coldplay with special guest performers Beyonc√© and Bruno Mars'}]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1a0e30b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c5420f6e484db29f54823e28e7e010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feac40291ce34f1f899b9f86ca7f9953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('squad')\n",
    "theoretical_answers = [{'id': ex['id'], 'answers': ex['answers']} for ex in small_eval_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d2fb8698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\n",
      "{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
     ]
    }
   ],
   "source": [
    "print(predicted_answers[0])\n",
    "print(theoretical_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e1dfd346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "545449ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature['example_id']].append(idx)\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example['id']\n",
    "        context = example['context']\n",
    "        answers = []\n",
    "        \n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit   = end_logits[feature_index]\n",
    "            offsets     = features[feature_index]['offset_mapping']\n",
    "            \n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best-1 : -1].tolist()  # big n_best\n",
    "            end_indexes   = np.argsort(end_logit)[-1 : -n_best-1 : -1].tolist()  # big n_best\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    if (start_index > end_index) or (end_index - start_index + 1 > max_answer_length):\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        'text': context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        'logit_score': start_logit[start_index] + end_logit[end_index]\n",
    "                    })\n",
    "        \n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x['logit_score'])\n",
    "            predicted_answers.append({'id': example_id, 'prediction_text': best_answer['text']})\n",
    "        else:\n",
    "            predicted_answers.append({'id': example_id, 'prediction_text': ''})\n",
    "    \n",
    "    theoretical_answers = [{'id': ex['id'], 'answers': ex['answers']} for ex in examples]    \n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0263a1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a5f72a31544c4c80e795d12e8d3da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab458da",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b5215e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-cased'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "15a664d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22c17f2f5f94cb6a5e483c8bb61c7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e1234",
   "metadata": {},
   "source": [
    "token: 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1373ef37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f03a2a08854848bb520d534bff2c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6bc2dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    'bert-finetuned-squad',\n",
    "    evaluation_strategy='no',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=1e-2,\n",
    "    fp16=True,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4704b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11092' max='11092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11092/11092 17:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.003600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bert-finetuned-squad/checkpoint-11092 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11092, training_loss=1.2305639784902689, metrics={'train_runtime': 1028.6571, 'train_samples_per_second': 86.257, 'train_steps_per_second': 10.783, 'total_flos': 1.7388449946321408e+16, 'train_loss': 1.2305639784902689, 'epoch': 1.0})"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a03c6a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dd528b07824552b1658a90f5de92a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 80.21759697256385, 'f1': 87.63448875058538}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(start_logits, end_logits), *_ = trainer.predict(validation_dataset)\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a8427ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Alchem/bert-finetuned-squad/tree/main/'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaebdd0",
   "metadata": {},
   "source": [
    "# A custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b431be",
   "metadata": {},
   "source": [
    "## Preparing everything for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7c57cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataset.set_format('torch')\n",
    "validation_set = validation_dataset.remove_columns(['example_id', 'offset_mapping'])\n",
    "validation_set.set_format('torch')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=8\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_set,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "077e4455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d9f20560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "062f34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5d79a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8074d56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alchem/bert-finetuned-squad-accelerate'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = 'bert-finetuned-squad-accelerate'\n",
    "repo_name  = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "74c23d71",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/huggingface-gHi8t1rX-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/huggingface-gHi8t1rX-py3.10/lib/python3.10/site-packages/huggingface_hub/repository.py:591\u001b[0m, in \u001b[0;36mRepository.check_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     lfs_version \u001b[38;5;241m=\u001b[39m \u001b[43mrun_subprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgit-lfs --version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/huggingface-gHi8t1rX-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_subprocess.py:83\u001b[0m, in \u001b[0;36mrun_subprocess\u001b[0;34m(command, folder, check, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(folder)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if not utf-8, replace char by ÔøΩ\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[226], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-finetuned-sqaud-accelerate\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m repo \u001b[38;5;241m=\u001b[39m \u001b[43mRepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/huggingface-gHi8t1rX-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:128\u001b[0m, in \u001b[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     warning_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message\n\u001b[1;32m    127\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/huggingface-gHi8t1rX-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/huggingface-gHi8t1rX-py3.10/lib/python3.10/site-packages/huggingface_hub/repository.py:521\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_lfs_files \u001b[38;5;241m=\u001b[39m skip_lfs_files\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m client \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m HfApi()\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_git_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuggingface_token: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/huggingface-gHi8t1rX-py3.10/lib/python3.10/site-packages/huggingface_hub/repository.py:593\u001b[0m, in \u001b[0;36mRepository.check_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m     lfs_version \u001b[38;5;241m=\u001b[39m run_subprocess(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit-lfs --version\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir)\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooks like you do not have git-lfs installed, please install.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can install from https://git-lfs.github.com/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Then run `git lfs install` (you only have to do this once).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m     )\n\u001b[1;32m    598\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(git_version \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m lfs_version)\n",
      "\u001b[0;31mOSError\u001b[0m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once)."
     ]
    }
   ],
   "source": [
    "output_dir = 'bert-finetuned-sqaud-accelerate'\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815aba24",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa41be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits   = []\n",
    "    accerlator.print(\"Evaluation!\")\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "        start_logits.append(accerlator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accerlator.gather(outputs.end_logits).cpu().numpy()) \n",
    "    \n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits   = np.concatenate(end_logits)\n",
    "    \n",
    "    start_logits = start_logits[:len(validation_dataset)]\n",
    "    end_logits   = end_logits[:len(validation_dataset)]\n",
    "    \n",
    "    metrics = compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets['validation'])\n",
    "    print(f\"epoch {epoch}: {metrics}\")\n",
    "    \n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428532fa",
   "metadata": {},
   "source": [
    "# Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d2b004f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d72a43cea34da3a837ec2f773154aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a1d78d1df743e1908f68018ecad54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba213f8901f948398b4be925ab15f83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b0e515c73648dbb1566ee4c2f64727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99b9b4a4c2846a18180471a41c58a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60b2ae69dd0446995daeb23ab0f82a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9340673089027405,\n",
       " 'start': 78,\n",
       " 'end': 105,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Alchem/bert-finetuned-squad\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ü§ó Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
